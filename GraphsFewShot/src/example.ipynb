{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "import math\n",
    "import time\n",
    "import numpy as np\n",
    "import json\n",
    "import random\n",
    "\n",
    "from model import FewShotModel\n",
    "from dataloader import Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Setting Up argument parser as a class for this example. While running the actual main.py file,\n",
    "argparse can be directly utilized for command line argumnts\n",
    "'''\n",
    "class Args:\n",
    "    dataset_name = \"TRIANGLES\"\n",
    "    device = 0\n",
    "    num_layers = 5\n",
    "    num_mlp_layers = 2\n",
    "    hidden_dim = 128\n",
    "    final_dropout = 0.5\n",
    "    graph_pooling_type = \"sum\"\n",
    "    neighbor_pooling_type = \"sum\"\n",
    "    learn_eps = True\n",
    "    degree_as_tag = True\n",
    "    \n",
    "    num_gat_layers = 2\n",
    "    gat_out_dim = 128\n",
    "    gat_dropout = 0.5\n",
    "    gat_heads = 2\n",
    "    gat_leaky_slope = 0.1\n",
    "    gat_concat = 0\n",
    "    \n",
    "    is_train = 1\n",
    "    is_test = 1\n",
    "    \n",
    "    batch_size = 64\n",
    "    iters_per_epoch = 10\n",
    "    epochs = 50\n",
    "    lr = 0.001\n",
    "    weight_decay = 1e-7\n",
    "    save_model_after = 1\n",
    "    n_shot = 20\n",
    "    model_runs = 1\n",
    "    knn_value = 2\n",
    "    train_clusters = 3\n",
    "    num_inference_graphs = 500\n",
    "    \n",
    "    fine_tune_lr = 0.001\n",
    "    fine_tune_weight_decay = 1e-7\n",
    "    fine_tune_epochs = 20\n",
    "    fine_tune_iters_per_epoch = 10\n",
    "    fine_tune_batch_size = 500\n",
    "    \n",
    "    fine_tune_save_model_after = 1\n",
    "    num_testing_runs = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "args = Args()\n",
    "\n",
    "knn_params = {\"initial\": args.knn_value}\n",
    "gat_layer_params = {}\n",
    "\n",
    "for i in range(args.num_gat_layers):\n",
    "    knn_params[i] = args.knn_value\n",
    "\n",
    "    gat_layer_params[i] = {}\n",
    "    if i == 0:\n",
    "        gat_layer_params[i][\"in_channels\"] = args.hidden_dim * (args.num_layers-1)\n",
    "        gat_layer_params[i][\"out_channels\"] = args.gat_out_dim\n",
    "\n",
    "    else:\n",
    "        if args.gat_concat == 1:\n",
    "            gat_layer_params[i][\"in_channels\"] = gat_layer_params[i-1][\"out_channels\"] * \\\n",
    "                                                 gat_layer_params[i-1][\"heads\"]\n",
    "        else:\n",
    "            gat_layer_params[i][\"in_channels\"] = gat_layer_params[i-1][\"out_channels\"]\n",
    "        gat_layer_params[i][\"out_channels\"] = args.gat_out_dim\n",
    "\n",
    "    if args.gat_concat == 1:\n",
    "        gat_layer_params[i][\"concat\"] = True\n",
    "    else:\n",
    "        gat_layer_params[i][\"concat\"] = False\n",
    "    gat_layer_params[i][\"heads\"] = args.gat_heads\n",
    "    gat_layer_params[i][\"leaky_slope\"] = args.gat_leaky_slope\n",
    "    gat_layer_params[i][\"dropout\"] = args.gat_dropout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda:\" + str(0)) if torch.cuda.is_available() else torch.device(\"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Current run =  1 \n",
      "\n",
      "\n",
      "Number of super clusters =  3 \n",
      "\n",
      "loading data\n",
      "# classes: 10\n",
      "# maximum node tag: 10\n",
      "# data: 2010 \n",
      "\n",
      "Loading class splits\n",
      "Number of training graphs =  1126\n",
      "Number of validation graphs =  281\n",
      "Number of total testing graphs =  603 \n",
      "\n",
      "\n",
      "Number of test fine tuning graphs =  60\n",
      "Number of test evaluation graphs =  543 \n",
      "\n",
      "average loss for training epoch  0  =  1.8963455200195312\n",
      "average loss for training epoch  1  =  1.793415641784668\n",
      "Saving Main Model in training phase\n",
      "average loss for training epoch  2  =  1.673684310913086\n",
      "Saving Main Model in training phase\n",
      "average loss for training epoch  3  =  1.5658200263977051\n",
      "Saving Main Model in training phase\n",
      "average loss for training epoch  4  =  1.4544219017028808\n",
      "Saving Main Model in training phase\n",
      "average loss for training epoch  5  =  1.4357110023498536\n",
      "Saving Main Model in training phase\n",
      "average loss for training epoch  6  =  1.3649810791015624\n",
      "Saving Main Model in training phase\n",
      "average loss for training epoch  7  =  1.3575006484985352\n",
      "Saving Main Model in training phase\n",
      "average loss for training epoch  8  =  1.382136631011963\n",
      "Saving Main Model in training phase\n",
      "average loss for training epoch  9  =  1.2857161521911622\n",
      "Saving Main Model in training phase\n",
      "average loss for training epoch  10  =  1.3095396041870118\n",
      "Saving Main Model in training phase\n",
      "average loss for training epoch  11  =  1.3072683334350585\n",
      "Saving Main Model in training phase\n",
      "average loss for training epoch  12  =  1.258288288116455\n",
      "Saving Main Model in training phase\n",
      "average loss for training epoch  13  =  1.2667169570922852\n",
      "Saving Main Model in training phase\n",
      "average loss for training epoch  14  =  1.1998308181762696\n",
      "Saving Main Model in training phase\n",
      "average loss for training epoch  15  =  1.2464205741882324\n",
      "Saving Main Model in training phase\n",
      "average loss for training epoch  16  =  1.2736759185791016\n",
      "Saving Main Model in training phase\n",
      "average loss for training epoch  17  =  1.2218894958496094\n",
      "Saving Main Model in training phase\n",
      "average loss for training epoch  18  =  1.1796598434448242\n",
      "Saving Main Model in training phase\n",
      "average loss for training epoch  19  =  1.2160346984863282\n",
      "Saving Main Model in training phase\n",
      "average loss for training epoch  20  =  1.215974998474121\n",
      "Saving Main Model in training phase\n",
      "average loss for training epoch  21  =  1.2191760063171386\n",
      "Saving Main Model in training phase\n",
      "average loss for training epoch  22  =  1.2336599349975585\n",
      "Saving Main Model in training phase\n",
      "average loss for training epoch  23  =  1.1915751457214356\n",
      "Saving Main Model in training phase\n",
      "average loss for training epoch  24  =  1.192233180999756\n",
      "Saving Main Model in training phase\n",
      "average loss for training epoch  25  =  1.1412261962890624\n",
      "Saving Main Model in training phase\n",
      "average loss for training epoch  26  =  1.1153186798095702\n",
      "Saving Main Model in training phase\n",
      "average loss for training epoch  27  =  1.1363395690917968\n",
      "Saving Main Model in training phase\n",
      "average loss for training epoch  28  =  1.1457630157470704\n",
      "Saving Main Model in training phase\n",
      "average loss for training epoch  29  =  1.1731724739074707\n",
      "Saving Main Model in training phase\n",
      "average loss for training epoch  30  =  1.1989496231079102\n",
      "Saving Main Model in training phase\n",
      "average loss for training epoch  31  =  1.134127140045166\n",
      "Saving Main Model in training phase\n",
      "average loss for training epoch  32  =  1.1435443878173828\n",
      "Saving Main Model in training phase\n",
      "average loss for training epoch  33  =  1.1462852478027343\n",
      "Saving Main Model in training phase\n",
      "average loss for training epoch  34  =  1.1209500312805176\n",
      "Saving Main Model in training phase\n",
      "average loss for training epoch  35  =  1.0865224838256835\n",
      "Saving Main Model in training phase\n",
      "average loss for training epoch  36  =  1.1237427711486816\n",
      "Saving Main Model in training phase\n",
      "average loss for training epoch  37  =  1.1696104049682616\n",
      "Saving Main Model in training phase\n",
      "average loss for training epoch  38  =  1.1124937057495117\n",
      "Saving Main Model in training phase\n",
      "average loss for training epoch  39  =  1.059275245666504\n",
      "Saving Main Model in training phase\n",
      "average loss for training epoch  40  =  1.160487174987793\n",
      "Saving Main Model in training phase\n",
      "average loss for training epoch  41  =  1.1606562614440918\n",
      "Saving Main Model in training phase\n",
      "average loss for training epoch  42  =  1.1529659271240233\n",
      "Saving Main Model in training phase\n",
      "average loss for training epoch  43  =  1.1030162811279296\n",
      "Saving Main Model in training phase\n",
      "average loss for training epoch  44  =  1.1132326126098633\n",
      "Saving Main Model in training phase\n",
      "average loss for training epoch  45  =  1.1698080062866212\n",
      "Saving Main Model in training phase\n",
      "average loss for training epoch  46  =  1.1550095558166504\n",
      "Saving Main Model in training phase\n",
      "average loss for training epoch  47  =  1.0802034378051757\n",
      "Saving Main Model in training phase\n",
      "average loss for training epoch  48  =  1.1083665847778321\n",
      "Saving Main Model in training phase\n",
      "average loss for training epoch  49  =  1.0798057556152343\n",
      "Saving Main Model in training phase\n",
      "average loss for training epoch  50  =  1.0830947875976562\n",
      "Saving Main Model in training phase\n",
      "\n",
      "Remaining Training phase finished in  19.414808988571167  seconds\n",
      "\n",
      "\n",
      "Fine tuning GIN for super-class classification on test classes...\n",
      "\n",
      "GIN and GAT Modules loaded for fine tuning\n",
      "Average loss for fine-tuning epoch  0  =  0.9743800163269043\n",
      "Average loss for fine-tuning epoch  1  =  0.7822272777557373\n",
      "Saving Main Model for fine tuning\n",
      "Average loss for fine-tuning epoch  2  =  0.6439130783081055\n",
      "Saving Main Model for fine tuning\n",
      "Average loss for fine-tuning epoch  3  =  0.5179679870605469\n",
      "Saving Main Model for fine tuning\n",
      "Average loss for fine-tuning epoch  4  =  0.4130853176116943\n",
      "Saving Main Model for fine tuning\n",
      "Average loss for fine-tuning epoch  5  =  0.37667629718780515\n",
      "Saving Main Model for fine tuning\n",
      "Average loss for fine-tuning epoch  6  =  0.3177053451538086\n",
      "Saving Main Model for fine tuning\n",
      "Average loss for fine-tuning epoch  7  =  0.3569946765899658\n",
      "Saving Main Model for fine tuning\n",
      "Average loss for fine-tuning epoch  8  =  0.34136526584625243\n",
      "Saving Main Model for fine tuning\n",
      "Average loss for fine-tuning epoch  9  =  0.30096871852874757\n",
      "Saving Main Model for fine tuning\n",
      "Average loss for fine-tuning epoch  10  =  0.2828260660171509\n",
      "Saving Main Model for fine tuning\n",
      "Average loss for fine-tuning epoch  11  =  0.3095591068267822\n",
      "Saving Main Model for fine tuning\n",
      "Average loss for fine-tuning epoch  12  =  0.3173747301101685\n",
      "Saving Main Model for fine tuning\n",
      "Average loss for fine-tuning epoch  13  =  0.2871126651763916\n",
      "Saving Main Model for fine tuning\n",
      "Average loss for fine-tuning epoch  14  =  0.2849758148193359\n",
      "Saving Main Model for fine tuning\n",
      "Average loss for fine-tuning epoch  15  =  0.27153451442718507\n",
      "Saving Main Model for fine tuning\n",
      "Average loss for fine-tuning epoch  16  =  0.2900233268737793\n",
      "Saving Main Model for fine tuning\n",
      "Average loss for fine-tuning epoch  17  =  0.2499298095703125\n",
      "Saving Main Model for fine tuning\n",
      "Average loss for fine-tuning epoch  18  =  0.27288379669189455\n",
      "Saving Main Model for fine tuning\n",
      "Average loss for fine-tuning epoch  19  =  0.27884759902954104\n",
      "Saving Main Model for fine tuning\n",
      "Average loss for fine-tuning epoch  20  =  0.2773072957992554\n",
      "Saving Main Model for fine tuning\n",
      "\n",
      "Fine tuning finished in  5.417916297912598  seconds\n",
      "\n",
      "Loaded model for testing\n",
      "Current run Testing accuracy =  0.798\n",
      "\n",
      "Total time taken for 1 complete run =  127.5093321800232  seconds\n",
      "\n",
      "\n",
      "Average acc over the model runs =  0.798  , std =  0.0\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "Creating Instance of the model here\n",
    "'''\n",
    "\n",
    "overall_acc = []\n",
    "for runs in range(args.model_runs):\n",
    "    print(\"\\nCurrent run = \", runs+1, \"\\n\")\n",
    "    main_start_time = time.time()\n",
    "    \n",
    "    dataset = Dataset(args.dataset_name, args)\n",
    "    dataset.segregate(args, runs)\n",
    "    \n",
    "    if args.is_train == 1:\n",
    "        train_start_time = time.time()\n",
    "\n",
    "        train_start_time = time.time()\n",
    "        few_shot_model = FewShotModel(args.num_layers, args.num_mlp_layers, \n",
    "                dataset.train_graphs[0].node_features.shape[1], args.hidden_dim, \n",
    "                args.train_clusters,\n",
    "                args.final_dropout, args.learn_eps, args.graph_pooling_type, \n",
    "                args.neighbor_pooling_type, device, args.num_gat_layers, gat_layer_params, \n",
    "                knn_params, dataset, \"train\").to(device)\n",
    "\n",
    "        few_shot_model.train_(args, dataset)\n",
    "        print(\"\\nRemaining Training phase finished in \", time.time() - train_start_time, \" seconds\\n\")\n",
    "        \n",
    "    if args.is_test == 1:\n",
    "        print(\"\\nFine tuning GIN for super-class classification on test classes...\\n\")\n",
    "        test_start_time = time.time()\n",
    "\n",
    "        few_shot_model = FewShotModel(args.num_layers, args.num_mlp_layers, \n",
    "                dataset.train_graphs[0].node_features.shape[1], args.hidden_dim, \n",
    "                args.train_clusters,\n",
    "                args.final_dropout, args.learn_eps, args.graph_pooling_type, \n",
    "                args.neighbor_pooling_type, device, args.num_gat_layers, gat_layer_params, \n",
    "                knn_params, dataset, \"test\").to(device)\n",
    "\n",
    "        few_shot_model.test_fine_tuning(args, dataset, device)\n",
    "        print(\"\\nFine tuning finished in \", time.time() - test_start_time, \" seconds\\n\")\n",
    "        current_run_acc = few_shot_model.test(args, dataset, device)\n",
    "        overall_acc.append(current_run_acc)\n",
    "        \n",
    "    print(\"\\nTotal time taken for 1 complete run = \", time.time() - main_start_time, \" seconds\\n\")\n",
    "    \n",
    "print(\"\\nAverage acc over the model runs = \", np.mean(overall_acc), \" , std = \", np.std(overall_acc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
